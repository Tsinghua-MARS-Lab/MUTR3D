<!doctype html>
<html lang="en">

<!-- === Header Starts === -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>MUTR3D</title>

    <link href="./assets/bootstrap.min.css" rel="stylesheet">
    <link href="./assets/font.css" rel="stylesheet" type="text/css">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
    <script src="./assets/jquery.min.js"></script>
    <script type="text/javascript" src="assets/corpus.js"></script>

</head>
<!-- === Header Ends === -->

<script>
    var lang_flag = 1;
</script>

<body>

<!-- === Home Section Starts === -->
<div class="section">
    <!-- === Title Starts === -->

    <div class="logo" align="center">
        <!-- <a href="" target="_blank"> -->
            <img style=" width: 400pt;" src="images/mutr3d_logo.png">
        <!-- </a> -->
    </div>

    <div class="header">
        <div style="" class="title" id="lang">
            <b>MUTR3D</b>: A Multi-camera Tracking Framework via 3D-to-2D Queries
        </div>
    </div>
    <!-- === Title Ends === -->

    <div class="author" style="margin-top: -30pt">
        <a href="https://tianyuanzhang.com/" target="_blank">Tianyuan Zhang</a><sup>1</sup>,&nbsp;
        <a href="#" target="_blank">Xuanyao Chen</a><sup>2</sup>,&nbsp;
        <a href="https://people.csail.mit.edu/yuewang/" target="_blank">Yue Wang</a><sup>3</sup>,&nbsp;
        <a href="https://scholar.google.com.hk/citations?hl=en&user=nUyTDosAAAAJ">Yilun Wang</a><sup>4</sup>,&nbsp;
        <a href="https://hangzhaomit.github.io/">Hang Zhao</a><sup>5</sup>&nbsp;
    </div>

    <div class="institution">
        <div><sup>1</sup>CMU,
            <sup>2</sup>Fudan University,
            <sup>3</sup>MIT,
            <sup>4</sup>Li Auto,
            <sup>5</sup>Tsinghua University
        </div>
        <div>
            CVPR 2022 Workshop on Autonomous Driving (formally published)
        </div>
    </div>

    <table border="0" align="center">
        <tr>
            <td align="center" style="padding: 0pt 0 15pt 0">
                <a class="bar" href="https://tsinghua-mars-lab.github.io/MUTR3D/"><b>Webpage</b></a> |
                <a class="bar" href="https://github.com/a1600012888/MUTR3D"><b>Code</b></a> |
                <a class="bar" href="https://arxiv.org/abs/2205.00613"><b>Paper</b></a>
            </td>
        </tr>
    </table>
    
    <!--<div align="center">
        <table width="100%" style="margin: 0pt 0pt; text-align: center;">
            <tr>
                <td>
                    <video style="display:block; width:100%; height:auto; "
                           autoplay="autoplay" muted loop="loop" controls playsinline>
                        <source src="https://raw.githubusercontent.com/decisionforce/archive/master/MetaDrive/metadrive_teaser.mp4"
                                type="video/mp4"/>
                    </video>
                </td>
            </tr>
        </table>
    </div> -->
    <p>
        An end-to-end multi-camera 3D tracking framework that works with arbrtary camera rigs with known parameters. MUTR3D handles multi-camera 3D detection, and cross-camera, cross-frame objects association in an end-to-end fashion.
    </p>
    
</div>
<!-- === Home Section Ends === -->




<div class="section">
    <div class="title" id="lang">Abstract</div>
    <p>
     Accurate and consistent 3D tracking from multiple cameras is a key component in a vision-based autonomous driving system. It involves modeling 3D dynamic objects in complex scenes across multiple cameras. This problem is inherently challenging due to depth estimation, visual occlusions, appearance ambiguity, etc. Moreover, objects are not consistently associated across time and cameras. To address that, we propose an end-to-end <b>MU</b>lti-camera <b>TR</b>acking framework called MUTR3D. In contrast to prior works, MUTR3D does not explicitly rely on the spatial and appearance similarity of objects. Instead, our method introduces 3D track query to model spatial and appearance coherent track for each object that appears in multiple cameras and multiple frames. We use camera transformations to link 3D trackers with their observations in 2D images. Each tracker is further refined according to the features that are obtained from camera images. MUTR3D uses a set-to-set loss to measure the difference between the predicted tracking results and the ground truths. Therefore, it does not require any post-processing such as non-maximum suppression and/or bounding box association. MUTR3D outperforms state-of-the-art methods by <b>5.3</b> AMOTA on the nuScenes dataset.
    </p>
    <!-- <ul>
        <li>
            <b>Compositional</b>: It supports generating infinite scenes with various road maps and traffic settings for the
            research of generalizable RL.
        </li>
        <li>
            <b>Lightweight</b>: It is easy to install and run. It can run up to 300 FPS on a standard PC.
        </li>
        <li>
            <b>Realistic</b>: Accurate physics simulation and multiple sensory input including Lidar, RGB images, top-down
            semantic map and first-person view images. The real traffic data replay is also supported.
        </li>
    </ul> -->
 
</div>

<div class="section">
    <div class="title" id="lang">Method</div>
    <div class="logo" style="" align="center">
        <img style="width: 700pt;" src="images/MUTR3D.png">
    </div>
    <p>
    <b>Query-based 3D tracking.</b> Query-based tracking is extended from query-based detection, where detect queries, a fixed-size set of embedding, are used to represent 2D object candidates. Track query extends the concept of the detect query to multi-frames, i.e., representing a whole tracklet across frames. Specifically, we initialize a set of newborn queries at the beginning of each frame, then queries update themselves frame-by-frame in an auto-regressive way. A decoder head predicts one object candidate from each track query in each frame, and boxes decoded in different frames from the same track query are directly associated. With proper query life cycle management, query-based tracking can perform joint detection and track in an online fashion. 
    </p>
<p>There are three key ingredients in our query-based multi-camera 3D tracker, as shown in above figure.</p>
<ul>
<li>A query-based object tracking loss assigns different regression targets for two different types of queries, newborn queries, and old queries. </li>
<li>A multi-camera sparse attention uses 3D reference points to sample image features for each query.</li>  
<li>A motion model estimates object dynamics and updates the query's reference point across frames. </li> 
</ul>
    
</div>


<div class="section">
    <div class="title" id="lang">Related Projects on <a href="https://tsinghua-mars-lab.github.io/vcad/">VCAD (Vision-Centric Autonomous Driving)</a></div>
    <div class="col text-center">

    <table width="100%" style="margin: 0pt 0pt; text-align: center;">
    <tr>
      <td>
      BEV Mapping<br>
      <a href="https://tsinghua-mars-lab.github.io/HDMapNet/" class="d-inline-block p-3"><img height="100"
          src="images/hdmapnet_thumbnail.gif" style="border:1px solid" data-nothumb><br>HDMapNet</a>
      </td>
        
      <td>
        BEV Vectorized Mapping<br>
        <a href="https://tsinghua-mars-lab.github.io/vectormapnet/" class="d-inline-block p-3"><img height="100"
            src="images/VectorMapNet_thumbnail.png" style="border:1px solid"
            data-nothumb><br>VectorMapNet</a>
      </td>

      <td>
      BEV Detection<br>
        <a href="https://tsinghua-mars-lab.github.io/detr3d/" class="d-inline-block p-3"><img height="100"
          src="images/detr3d_thumbnail.png" style="border:1px solid"
          data-nothumb><br>DETR3D</a>
      </td>

      <td>
      BEV Fusion<br>
        <a href="https://tsinghua-mars-lab.github.io/futr3d/" class="d-inline-block p-3"><img height="100"
          src="images/futr3d_thumbnail.png" style="border:1px solid"
          data-nothumb><br>FUTR3D</a>
      </td>

    </tr>

    </table>
    </div>

    
</div>

<!-- === Reference Section Starts === -->
<div class="section">
    <div class="bibtex">
       <div class="title" id="lang">Reference</div>
    </div>
<p>If you find our work useful in your research, please cite our paper:</p>
    <pre>
@article{zhang2022mutr3d,
  title={MUTR3D: A Multi-camera Tracking Framework via 3D-to-2D Queries},
  author={Zhang, Tianyuan and Chen, Xuanyao and Wang, Yue and Wang, Yilun and Zhao, Hang},
  journal={arXiv preprint arXiv:2205.00613},
  year={2022}
}
</pre>
    <!-- Adjust the frame size based on the demo (Every project differs). -->
</div>

</body>
</html>
